{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.decomposition import PCA\n",
    "import torch.utils.data\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import gc\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "set parameters\n",
    "'''\n",
    "base_path = '.'\n",
    "train_path = base_path + '/train.npy'\n",
    "test_path = base_path + '/test.npy'\n",
    "train_labels_path = base_path + '/train_labels.npy'\n",
    "dev_labels_path = base_path +  '/dev_labels.npy'\n",
    "dev_path = base_path + '/dev.npy'\n",
    "\n",
    "pca_available = True\n",
    "padding_method = 'self' \n",
    "# padding_method = 'zero' \n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "n_labels = 138\n",
    "n_features = 40\n",
    "n_epoch = 10\n",
    "context_num = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data():\n",
    "    t0 = time.time()\n",
    "    print(\"Start loading training data...\")\n",
    "    train = np.load(train_path, allow_pickle=True)\n",
    "    train_labels = np.load(train_labels_path, allow_pickle=True)\n",
    "    t1 = time.time()\n",
    "    elapsed_time = t1 - t0\n",
    "    print(\"Done loading training data in {0} minutes...\".format(elapsed_time/60))\n",
    "    \n",
    "    return train, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_validation_data():\n",
    "    t0 = time.time()\n",
    "    print(\"Start loading validation data...\")\n",
    "    val = np.load(dev_path, allow_pickle=True)\n",
    "    val_labels = np.load(dev_labels_path, allow_pickle=True)\n",
    "    t1 = time.time()\n",
    "    elapsed_time = t1 - t0\n",
    "    print(\"Done loading validation data in {0} minutes...\".format(elapsed_time/60))\n",
    "    \n",
    "    return val, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data():\n",
    "    t0 = time.time()\n",
    "    print(\"Start loading test data...\")\n",
    "    test = np.load(test_path, allow_pickle=True)\n",
    "    t1 = time.time()\n",
    "    elapsed_time = t1 - t0\n",
    "    print(\"Done loading test data in {0} minutes...\".format(elapsed_time/60))\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(features, labels, pca, context_num):\n",
    "    '''\n",
    "    use the first&last frame of one utterance to pad the empty frame\n",
    "    '''\n",
    "    t0 = time.time()\n",
    "    padding_features = np.concatenate([np.concatenate(( \\\n",
    "                                                 np.ones((context_num, pca.n_components))*pca.transform(features[i][[0]]), \\\n",
    "                                                 pca.transform(features[i]), \\\n",
    "                                                 np.ones((context_num, pca.n_components))*pca.transform(features[i][[-1]]))) \\\n",
    "                                                 for i in range(len(features))])\n",
    "    padding_features = torch.Tensor(padding_features)\n",
    "    del features\n",
    "    \n",
    "    '''\n",
    "    corresponding label for padding frames\n",
    "    '''\n",
    "    false_labels = np.array([-1]*context_num)\n",
    "    padding_labels = np.concatenate([np.concatenate(( \\\n",
    "                                                  false_labels, \\\n",
    "                                                  labels[i], \\\n",
    "                                                  false_labels)) \\\n",
    "                                                  for i in range(len(labels))])\n",
    "    padding_labels = torch.Tensor(padding_labels)\n",
    "    del labels\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return padding_features, padding_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, context_num, features, targets):\n",
    "        \n",
    "        self.context_num = context_num\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        if index-self.context_num >= 0 and index+self.context_num+1 <= len(self.targets)-1:\n",
    "            '''\n",
    "            no need for padding\n",
    "            '''\n",
    "            X = self.features[index-self.context_num: index+self.context_num+1].reshape(-1)\n",
    "            Y = self.targets[index].long()\n",
    "        elif index-self.context_num < 0:\n",
    "            '''\n",
    "            padding for pre frames, actually doesnt matter since we drop this 'false' frame\n",
    "            '''\n",
    "            X = torch.cat((torch.zeros(self.context_num-index, self.features.shape[1]), self.features[:index+self.context_num+1]), 0).reshape(-1)\n",
    "            Y = self.targets[index].long()\n",
    "        else:\n",
    "            '''\n",
    "            padding for post frames, same as before\n",
    "            '''\n",
    "            X = torch.cat((self.features[index-self.context_num:], torch.zeros(index+self.context_num+1-len(self.targets), self.features.shape[1])),0).reshape(-1)\n",
    "            Y = self.targets[index].long()\n",
    "        \n",
    "        return index, X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1000,2048,1024,512,256+2,138\n",
    "class SpeechNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, context_num):\n",
    "        \n",
    "        super(SpeechNet, self).__init__()\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.relu7 = nn.ReLU()\n",
    "        self.relu8 = nn.ReLU()\n",
    "        \n",
    "        self.linear1 = nn.Linear((2*context_num+1)*pca.n_components, 2048)\n",
    "        self.linear2 = nn.Linear(2048, 1024)\n",
    "        self.linear3 = nn.Linear(1024, 810)\n",
    "        self.linear4 = nn.Linear(810, 720)\n",
    "        self.linear5 = nn.Linear(720, 512)\n",
    "        self.linear6 = nn.Linear(512, 428)\n",
    "        self.linear7 = nn.Linear(428, 300)\n",
    "        self.linear8 = nn.Linear(300, 256)\n",
    "        self.out = nn.Linear(256+2, 138)\n",
    "        \n",
    "        self.batchnorm1 = nn.BatchNorm1d(2048)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(1024)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(810)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(720)\n",
    "        self.batchnorm5 = nn.BatchNorm1d(512)\n",
    "        self.batchnorm6 = nn.BatchNorm1d(428)\n",
    "        self.batchnorm7 = nn.BatchNorm1d(300)\n",
    "        self.batchnorm8 = nn.BatchNorm1d(256+2)\n",
    "        \n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.05)\n",
    "        self.dropout3 = nn.Dropout(0.1)\n",
    "        self.dropout4 = nn.Dropout(0.05)\n",
    "        self.dropout5 = nn.Dropout(0.05)\n",
    "        self.dropout6 = nn.Dropout(0.05)\n",
    "        self.dropout7 = nn.Dropout(0.05)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu1(x)\n",
    "        #2048\n",
    "        x = self.dropout1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu2(x)\n",
    "        #1024\n",
    "        x = self.dropout2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.relu3(x)\n",
    "        #810\n",
    "        x = self.dropout3(x)\n",
    "        x = self.linear4(x)\n",
    "        x = self.batchnorm4(x)\n",
    "        x = self.relu4(x)\n",
    "        \n",
    "        #512\n",
    "        x = self.dropout4(x)\n",
    "        x = self.linear5(x)\n",
    "        x = self.batchnorm5(x)\n",
    "        x = self.relu5(x)\n",
    "        \n",
    "        #512\n",
    "        x = self.dropout5(x)\n",
    "        x = self.linear6(x)\n",
    "        x = self.batchnorm6(x)\n",
    "        x = self.relu6(x)\n",
    "        #428\n",
    "        x = self.dropout6(x)\n",
    "        x = self.linear7(x)\n",
    "        x = self.batchnorm7(x)\n",
    "        x = self.relu7(x)\n",
    "        #300 \n",
    "        x = self.dropout7(x)\n",
    "        x = self.linear8(x)\n",
    "        #300\n",
    "        \n",
    "        avg_pool1 = torch.mean(x, 1, keepdims = True)\n",
    "        max_pool1,_ = torch.max(x, 1, keepdims = True)\n",
    "        \n",
    "        conc = torch.cat((x, avg_pool1, max_pool1), 1)\n",
    "        conc = self.batchnorm8(conc)\n",
    "        output = self.out(conc)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(context_num, features, labels):\n",
    "    t0 = time.time()\n",
    "    print(\"It may takes 20 minutes to generate train dataset...\")\n",
    "    context_dataset = ContextDataset(context_num, features, labels)\n",
    "    dataset = SpeechDataset(context_dataset)\n",
    "    t1 = time.time()\n",
    "    print(\"Dataset generated. Elapsed time: {0}\".format((t1-t0)/60))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        xavier(m.weight.data)\n",
    "        xavier(m.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_cos(x):\n",
    "    start = 5e-3\n",
    "    end = 1e-5\n",
    "    return start + (1 + np.cos(np.pi * (1 - x))) * (end - start) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_scale_cos(x):\n",
    "    start = 1e-4\n",
    "    end = 1e-8\n",
    "    return start + (1 + np.cos(np.pi * (1 - x))) * (end - start) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamScheduler:\n",
    "    \n",
    "    def __init__(self, optimizer, scale_fn, total_steps):\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.scale_fn = scale_fn\n",
    "        self.total_steps = total_steps\n",
    "        self.current_iteration = 0\n",
    "        \n",
    "    def batch_step(self):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = self.scale_fn(self.current_iteration/self.total_steps)\n",
    "        \n",
    "        self.current_iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataloader, val_dataloader, n_epochs = 10):\n",
    "    \n",
    "    model = SpeechNet(context_num).to(device)\n",
    "    model.apply(weights_init)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "    \n",
    "    '''\n",
    "    set scheduler for decaying learning rate\n",
    "    '''\n",
    "    parameter_scheduler = ParamScheduler(optimizer, scale_cos, n_epoch*len(train_dataloader))\n",
    "    candidate_model = 1\n",
    "    print('Start training...')\n",
    "    for i in range(n_epochs):\n",
    "        \n",
    "        t0 = time.time()\n",
    "        \n",
    "        avg_loss_1000_batch = 0\n",
    "        val_correct = 0\n",
    "        val_predicted = 0\n",
    "        model.train()\n",
    "        \n",
    "        for index, (idx, features, labels) in enumerate(train_dataloader):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            mask = [i for i in range(len(labels)) if labels[i] != torch.Tensor([-1]).long()[0]]\n",
    "            features = features[mask].cuda()\n",
    "            labels = labels[mask].cuda()\n",
    "            '''\n",
    "            forward and backward\n",
    "            '''\n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels.long())\n",
    "            avg_loss_1000_batch += loss.item()\n",
    "            loss.backward()\n",
    "            \n",
    "            parameter_scheduler.batch_step()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if index % 2000 == 0 and index != 0:\n",
    "                \n",
    "                predictions = torch.max(output.data, 1)[1]\n",
    "                predicted = len(features)\n",
    "                correct = int(sum(predictions == labels.to(device)).cpu())\n",
    "                print(\"Epoch: {0}/{1} Train batch:{2}/{3}   acc: {4}  loss: {5}\".format(i+1, \\\n",
    "                                                                              n_epochs, \\\n",
    "                                                                              index, \\\n",
    "                                                                              len(train_dataloader), \\\n",
    "                                                                              correct/predicted, avg_loss_1000_batch/512))\n",
    "                avg_loss_1000_batch = 0\n",
    "                \n",
    "        for index, (idx, val_features, val_labels) in enumerate(val_dataloader):\n",
    "            \n",
    "            mask = [i for i in range(len(val_labels)) if val_labels[i] != torch.Tensor([-1])[0]]\n",
    "            val_features = val_features[mask].to(device)\n",
    "            val_labels = val_labels[mask].to(device)\n",
    "            model.eval()\n",
    "            outputs = model(val_features)\n",
    "            predictions = torch.max(outputs.data, 1)[1]\n",
    "            val_predicted += len(val_features)\n",
    "            val_correct += sum(predictions == val_labels.to(device))\n",
    "            \n",
    "        epoch_acc = int(val_correct.cpu())/val_predicted\n",
    "        if epoch_acc >= 0.68:\n",
    "            pickle.dump(model, open(\"candidate_model_{0}.pkl\".format(candidate_model), 'wb'))\n",
    "            print(\"Save one candidate model.\")\n",
    "            candidate_model += 1\n",
    "            \n",
    "        t1 = time.time()\n",
    "        print(\"Validation Accuracy: {0}. Cost time: {1} minutes\".format(int(val_correct.cpu())/val_predicted, (t1-t0)/60))\n",
    "        print(\"===================================================\")\n",
    "        \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading training data...\n",
      "Done loading training data in 0.7987695336341858 minutes...\n",
      "Start loading validation data...\n",
      "Done loading validation data in 0.0456453800201416 minutes...\n",
      "Start loading test data...\n",
      "Done loading test data in 0.014128851890563964 minutes...\n",
      "Start training...\n",
      "Epoch: 1/10 Train batch:2000/31779   acc: 0.45121951219512196  loss: 9.7974068808835\n",
      "Epoch: 1/10 Train batch:4000/31779   acc: 0.4979253112033195  loss: 8.131005998002365\n",
      "Epoch: 1/10 Train batch:6000/31779   acc: 0.5040816326530613  loss: 7.608875133330002\n",
      "Epoch: 1/10 Train batch:8000/31779   acc: 0.5708418891170431  loss: 7.294528134632856\n",
      "Epoch: 1/10 Train batch:10000/31779   acc: 0.5343035343035343  loss: 7.054817696334794\n",
      "Epoch: 1/10 Train batch:12000/31779   acc: 0.5429769392033543  loss: 6.877835999010131\n",
      "Epoch: 1/10 Train batch:14000/31779   acc: 0.5979381443298969  loss: 6.748973261332139\n",
      "Epoch: 1/10 Train batch:16000/31779   acc: 0.5637860082304527  loss: 6.626700401538983\n",
      "Epoch: 1/10 Train batch:18000/31779   acc: 0.56875  loss: 6.515335530042648\n",
      "Epoch: 1/10 Train batch:20000/31779   acc: 0.5379876796714579  loss: 6.44390314957127\n",
      "Epoch: 1/10 Train batch:22000/31779   acc: 0.5938775510204082  loss: 6.355136304395273\n",
      "Epoch: 1/10 Train batch:24000/31779   acc: 0.5967078189300411  loss: 6.303867116337642\n",
      "Epoch: 1/10 Train batch:26000/31779   acc: 0.5983263598326359  loss: 6.232847114559263\n",
      "Epoch: 1/10 Train batch:28000/31779   acc: 0.5479166666666667  loss: 6.187457967782393\n",
      "Epoch: 1/10 Train batch:30000/31779   acc: 0.5860655737704918  loss: 6.145368884084746\n",
      "Validation Accuracy: 0.6099275563894199. Cost time: 24.387852080663045 minutes\n",
      "===================================================\n",
      "Epoch: 2/10 Train batch:2000/31779   acc: 0.6356107660455487  loss: 6.011876984499395\n",
      "Epoch: 2/10 Train batch:4000/31779   acc: 0.5904365904365905  loss: 5.984314683126286\n",
      "Epoch: 2/10 Train batch:6000/31779   acc: 0.6290983606557377  loss: 5.945681227371097\n",
      "Epoch: 2/10 Train batch:8000/31779   acc: 0.5785123966942148  loss: 5.91851989692077\n",
      "Epoch: 2/10 Train batch:10000/31779   acc: 0.6042105263157894  loss: 5.88619122421369\n",
      "Epoch: 2/10 Train batch:12000/31779   acc: 0.6125  loss: 5.852636412251741\n",
      "Epoch: 2/10 Train batch:14000/31779   acc: 0.5871369294605809  loss: 5.832868226803839\n",
      "Epoch: 2/10 Train batch:16000/31779   acc: 0.5857142857142857  loss: 5.808117415057495\n",
      "Epoch: 2/10 Train batch:18000/31779   acc: 0.5995893223819302  loss: 5.789125464623794\n",
      "Epoch: 2/10 Train batch:20000/31779   acc: 0.59375  loss: 5.754060934996232\n",
      "Epoch: 2/10 Train batch:22000/31779   acc: 0.6470588235294118  loss: 5.73205794300884\n",
      "Epoch: 2/10 Train batch:24000/31779   acc: 0.6504065040650406  loss: 5.722947325091809\n",
      "Epoch: 2/10 Train batch:26000/31779   acc: 0.6164948453608248  loss: 5.697432388318703\n",
      "Epoch: 2/10 Train batch:28000/31779   acc: 0.6244813278008299  loss: 5.680661244317889\n",
      "Epoch: 2/10 Train batch:30000/31779   acc: 0.6149068322981367  loss: 5.653364775236696\n",
      "Validation Accuracy: 0.6388902041323635. Cost time: 24.138027866681416 minutes\n",
      "===================================================\n",
      "Epoch: 3/10 Train batch:2000/31779   acc: 0.6597938144329897  loss: 5.577822788618505\n",
      "Epoch: 3/10 Train batch:4000/31779   acc: 0.6701244813278008  loss: 5.539097403176129\n",
      "Epoch: 3/10 Train batch:6000/31779   acc: 0.6453608247422681  loss: 5.539288379950449\n",
      "Epoch: 3/10 Train batch:8000/31779   acc: 0.6278118609406953  loss: 5.518072393490002\n",
      "Epoch: 3/10 Train batch:10000/31779   acc: 0.6440329218106996  loss: 5.5066706200595945\n",
      "Epoch: 3/10 Train batch:12000/31779   acc: 0.6053169734151329  loss: 5.500768842641264\n",
      "Epoch: 3/10 Train batch:14000/31779   acc: 0.6529774127310062  loss: 5.48765030503273\n",
      "Epoch: 3/10 Train batch:16000/31779   acc: 0.6459627329192547  loss: 5.484318618895486\n",
      "Epoch: 3/10 Train batch:18000/31779   acc: 0.6523517382413088  loss: 5.469916230067611\n",
      "Epoch: 3/10 Train batch:20000/31779   acc: 0.6515463917525773  loss: 5.443030263995752\n",
      "Epoch: 3/10 Train batch:22000/31779   acc: 0.6040816326530613  loss: 5.427898246562108\n",
      "Epoch: 3/10 Train batch:24000/31779   acc: 0.6561181434599156  loss: 5.423127093818039\n",
      "Epoch: 3/10 Train batch:26000/31779   acc: 0.639344262295082  loss: 5.419284620322287\n",
      "Epoch: 3/10 Train batch:28000/31779   acc: 0.6378600823045267  loss: 5.411432175664231\n",
      "Epoch: 3/10 Train batch:30000/31779   acc: 0.663135593220339  loss: 5.395839522359893\n",
      "Validation Accuracy: 0.6519155534774709. Cost time: 24.139771298567453 minutes\n",
      "===================================================\n",
      "Epoch: 4/10 Train batch:2000/31779   acc: 0.6257668711656442  loss: 5.316210693912581\n",
      "Epoch: 4/10 Train batch:4000/31779   acc: 0.6548856548856549  loss: 5.294358380604535\n",
      "Epoch: 4/10 Train batch:6000/31779   acc: 0.5907172995780591  loss: 5.295945049962029\n",
      "Epoch: 4/10 Train batch:8000/31779   acc: 0.6321353065539113  loss: 5.287397424690425\n",
      "Epoch: 4/10 Train batch:10000/31779   acc: 0.5917525773195876  loss: 5.273880596272647\n",
      "Epoch: 4/10 Train batch:12000/31779   acc: 0.6605691056910569  loss: 5.276953265769407\n",
      "Epoch: 4/10 Train batch:14000/31779   acc: 0.6401673640167364  loss: 5.265228853095323\n",
      "Epoch: 4/10 Train batch:16000/31779   acc: 0.6384297520661157  loss: 5.254212558735162\n",
      "Epoch: 4/10 Train batch:18000/31779   acc: 0.6346555323590815  loss: 5.241534168599173\n",
      "Epoch: 4/10 Train batch:20000/31779   acc: 0.675564681724846  loss: 5.22733993944712\n",
      "Epoch: 4/10 Train batch:22000/31779   acc: 0.6567796610169492  loss: 5.215027284575626\n",
      "Epoch: 4/10 Train batch:24000/31779   acc: 0.6880165289256198  loss: 5.212054678238928\n",
      "Epoch: 4/10 Train batch:26000/31779   acc: 0.656964656964657  loss: 5.22141223680228\n",
      "Epoch: 4/10 Train batch:28000/31779   acc: 0.6869918699186992  loss: 5.199652779381722\n",
      "Epoch: 4/10 Train batch:30000/31779   acc: 0.6632860040567952  loss: 5.195865701651201\n",
      "Validation Accuracy: 0.6607150255387402. Cost time: 24.175887401898702 minutes\n",
      "===================================================\n",
      "Epoch: 5/10 Train batch:2000/31779   acc: 0.6170212765957447  loss: 5.096538502257317\n",
      "Epoch: 5/10 Train batch:4000/31779   acc: 0.654320987654321  loss: 5.117359784897417\n",
      "Epoch: 5/10 Train batch:6000/31779   acc: 0.6894409937888198  loss: 5.116731307236478\n",
      "Epoch: 5/10 Train batch:8000/31779   acc: 0.7052845528455285  loss: 5.098598741926253\n",
      "Epoch: 5/10 Train batch:10000/31779   acc: 0.65625  loss: 5.090086501324549\n",
      "Epoch: 5/10 Train batch:12000/31779   acc: 0.6460905349794238  loss: 5.0817518518306315\n",
      "Epoch: 5/10 Train batch:14000/31779   acc: 0.696969696969697  loss: 5.080493567278609\n",
      "Epoch: 5/10 Train batch:16000/31779   acc: 0.6275720164609053  loss: 5.0843684275168926\n",
      "Epoch: 5/10 Train batch:18000/31779   acc: 0.6659836065573771  loss: 5.072955843992531\n",
      "Epoch: 5/10 Train batch:20000/31779   acc: 0.6511156186612576  loss: 5.063052929705009\n",
      "Epoch: 5/10 Train batch:22000/31779   acc: 0.6880165289256198  loss: 5.05785480258055\n",
      "Epoch: 5/10 Train batch:24000/31779   acc: 0.694560669456067  loss: 5.0376209418755025\n",
      "Epoch: 5/10 Train batch:26000/31779   acc: 0.6912065439672802  loss: 5.037911129882559\n",
      "Epoch: 5/10 Train batch:28000/31779   acc: 0.651356993736952  loss: 5.0399528671987355\n",
      "Epoch: 5/10 Train batch:30000/31779   acc: 0.6495901639344263  loss: 5.034081088611856\n",
      "Validation Accuracy: 0.6682242437514426. Cost time: 24.154389715194704 minutes\n",
      "===================================================\n",
      "Epoch: 6/10 Train batch:2000/31779   acc: 0.609504132231405  loss: 4.947956583695486\n",
      "Epoch: 6/10 Train batch:4000/31779   acc: 0.6456211812627292  loss: 4.955424337415025\n",
      "Epoch: 6/10 Train batch:6000/31779   acc: 0.6715481171548117  loss: 4.939864028710872\n",
      "Epoch: 6/10 Train batch:8000/31779   acc: 0.6659836065573771  loss: 4.936348338378593\n",
      "Epoch: 6/10 Train batch:10000/31779   acc: 0.6743697478991597  loss: 4.935492197517306\n",
      "Epoch: 6/10 Train batch:12000/31779   acc: 0.6409185803757829  loss: 4.931298549287021\n",
      "Epoch: 6/10 Train batch:14000/31779   acc: 0.6340956340956341  loss: 4.934873427497223\n",
      "Epoch: 6/10 Train batch:16000/31779   acc: 0.6268041237113402  loss: 4.929511033347808\n",
      "Epoch: 6/10 Train batch:18000/31779   acc: 0.6902286902286903  loss: 4.9251540408004075\n",
      "Epoch: 6/10 Train batch:20000/31779   acc: 0.6757322175732218  loss: 4.921367508592084\n",
      "Epoch: 6/10 Train batch:22000/31779   acc: 0.689795918367347  loss: 4.916912987595424\n",
      "Epoch: 6/10 Train batch:24000/31779   acc: 0.7276507276507277  loss: 4.916759182233363\n",
      "Epoch: 6/10 Train batch:26000/31779   acc: 0.6735966735966736  loss: 4.895018240902573\n",
      "Epoch: 6/10 Train batch:28000/31779   acc: 0.6604938271604939  loss: 4.895448314957321\n",
      "Epoch: 6/10 Train batch:30000/31779   acc: 0.6997929606625258  loss: 4.893644623807631\n",
      "Validation Accuracy: 0.6722858800063921. Cost time: 24.207170498371124 minutes\n",
      "===================================================\n",
      "Epoch: 7/10 Train batch:2000/31779   acc: 0.6473029045643154  loss: 4.82361814728938\n",
      "Epoch: 7/10 Train batch:4000/31779   acc: 0.6871035940803383  loss: 4.8115820796228945\n",
      "Epoch: 7/10 Train batch:6000/31779   acc: 0.6911764705882353  loss: 4.816671787411906\n",
      "Epoch: 7/10 Train batch:8000/31779   acc: 0.709278350515464  loss: 4.808412558748387\n",
      "Epoch: 7/10 Train batch:10000/31779   acc: 0.676954732510288  loss: 4.8102868042187765\n",
      "Epoch: 7/10 Train batch:12000/31779   acc: 0.6514522821576764  loss: 4.803506872034632\n",
      "Epoch: 7/10 Train batch:14000/31779   acc: 0.65439672801636  loss: 4.798859849339351\n",
      "Epoch: 7/10 Train batch:16000/31779   acc: 0.6570841889117043  loss: 4.791102161863819\n",
      "Epoch: 7/10 Train batch:18000/31779   acc: 0.6687116564417178  loss: 4.789656708016992\n",
      "Epoch: 7/10 Train batch:20000/31779   acc: 0.6772823779193206  loss: 4.790836266707629\n",
      "Epoch: 7/10 Train batch:22000/31779   acc: 0.6940928270042194  loss: 4.789164402987808\n",
      "Epoch: 7/10 Train batch:24000/31779   acc: 0.6867219917012448  loss: 4.794918390456587\n",
      "Epoch: 7/10 Train batch:26000/31779   acc: 0.6804123711340206  loss: 4.764776983647607\n",
      "Epoch: 7/10 Train batch:28000/31779   acc: 0.6804979253112033  loss: 4.777038602274843\n",
      "Epoch: 7/10 Train batch:30000/31779   acc: 0.6639175257731958  loss: 4.764546650694683\n",
      "Validation Accuracy: 0.6772663782337727. Cost time: 24.20336433649063 minutes\n",
      "===================================================\n",
      "Epoch: 8/10 Train batch:2000/31779   acc: 0.6796714579055442  loss: 4.709960559150204\n",
      "Epoch: 8/10 Train batch:4000/31779   acc: 0.6818181818181818  loss: 4.698262932128273\n",
      "Epoch: 8/10 Train batch:6000/31779   acc: 0.675  loss: 4.705215996829793\n",
      "Epoch: 8/10 Train batch:8000/31779   acc: 0.681912681912682  loss: 4.721495227771811\n",
      "Epoch: 8/10 Train batch:10000/31779   acc: 0.6529774127310062  loss: 4.694871228188276\n",
      "Epoch: 8/10 Train batch:12000/31779   acc: 0.6611570247933884  loss: 4.7115567362634465\n",
      "Epoch: 8/10 Train batch:14000/31779   acc: 0.6762295081967213  loss: 4.6913792652776465\n",
      "Epoch: 8/10 Train batch:16000/31779   acc: 0.7098765432098766  loss: 4.691945286118425\n",
      "Epoch: 8/10 Train batch:18000/31779   acc: 0.7037037037037037  loss: 4.68904441955965\n",
      "Epoch: 8/10 Train batch:20000/31779   acc: 0.652542372881356  loss: 4.686819446622394\n",
      "Epoch: 8/10 Train batch:22000/31779   acc: 0.6550308008213552  loss: 4.699441241915338\n",
      "Epoch: 8/10 Train batch:24000/31779   acc: 0.6735112936344969  loss: 4.680031765834428\n",
      "Epoch: 8/10 Train batch:26000/31779   acc: 0.6855345911949685  loss: 4.680239063454792\n",
      "Epoch: 8/10 Train batch:28000/31779   acc: 0.6862348178137652  loss: 4.681143297930248\n",
      "Epoch: 8/10 Train batch:30000/31779   acc: 0.7018633540372671  loss: 4.674899903242476\n",
      "Validation Accuracy: 0.6798705603134488. Cost time: 24.238636338710783 minutes\n",
      "===================================================\n",
      "Epoch: 9/10 Train batch:2000/31779   acc: 0.6673596673596673  loss: 4.62523766933009\n",
      "Epoch: 9/10 Train batch:4000/31779   acc: 0.6777546777546778  loss: 4.633318156236783\n",
      "Epoch: 9/10 Train batch:6000/31779   acc: 0.6894409937888198  loss: 4.631312782526948\n",
      "Epoch: 9/10 Train batch:8000/31779   acc: 0.6927835051546392  loss: 4.628271773690358\n",
      "Epoch: 9/10 Train batch:10000/31779   acc: 0.7125  loss: 4.638095643138513\n",
      "Epoch: 9/10 Train batch:12000/31779   acc: 0.6900826446280992  loss: 4.626812071190216\n",
      "Epoch: 9/10 Train batch:14000/31779   acc: 0.689795918367347  loss: 4.634401623974554\n",
      "Epoch: 9/10 Train batch:16000/31779   acc: 0.7154639175257732  loss: 4.624055395019241\n",
      "Epoch: 9/10 Train batch:18000/31779   acc: 0.6834381551362684  loss: 4.624530459055677\n",
      "Epoch: 9/10 Train batch:20000/31779   acc: 0.6923076923076923  loss: 4.618407785426825\n",
      "Epoch: 9/10 Train batch:22000/31779   acc: 0.7134146341463414  loss: 4.614892411511391\n",
      "Epoch: 9/10 Train batch:24000/31779   acc: 0.6652977412731006  loss: 4.626754371216521\n",
      "Epoch: 9/10 Train batch:26000/31779   acc: 0.65625  loss: 4.615499716019258\n",
      "Epoch: 9/10 Train batch:28000/31779   acc: 0.6956521739130435  loss: 4.628636922338046\n",
      "Epoch: 9/10 Train batch:30000/31779   acc: 0.66875  loss: 4.606372268171981\n",
      "Save one candidate model.\n",
      "Validation Accuracy: 0.6809092738475014. Cost time: 24.235928801695504 minutes\n",
      "===================================================\n",
      "Epoch: 10/10 Train batch:2000/31779   acc: 0.6987704918032787  loss: 4.59963982575573\n",
      "Epoch: 10/10 Train batch:4000/31779   acc: 0.6653061224489796  loss: 4.593702975544147\n",
      "Epoch: 10/10 Train batch:6000/31779   acc: 0.6356107660455487  loss: 4.588591465493664\n",
      "Epoch: 10/10 Train batch:8000/31779   acc: 0.6604166666666667  loss: 4.586460459046066\n",
      "Epoch: 10/10 Train batch:10000/31779   acc: 0.6536082474226804  loss: 4.600192621466704\n",
      "Epoch: 10/10 Train batch:12000/31779   acc: 0.668041237113402  loss: 4.577259998070076\n",
      "Epoch: 10/10 Train batch:14000/31779   acc: 0.6939203354297694  loss: 4.5920589016750455\n",
      "Epoch: 10/10 Train batch:16000/31779   acc: 0.6831275720164609  loss: 4.590704471222125\n",
      "Epoch: 10/10 Train batch:18000/31779   acc: 0.6708860759493671  loss: 4.583518972620368\n",
      "Epoch: 10/10 Train batch:20000/31779   acc: 0.6694560669456067  loss: 4.592146473238245\n",
      "Epoch: 10/10 Train batch:22000/31779   acc: 0.6975308641975309  loss: 4.593045651330613\n",
      "Epoch: 10/10 Train batch:24000/31779   acc: 0.6548117154811716  loss: 4.589831111487001\n",
      "Epoch: 10/10 Train batch:26000/31779   acc: 0.6851851851851852  loss: 4.58333218190819\n",
      "Epoch: 10/10 Train batch:28000/31779   acc: 0.7063655030800822  loss: 4.609291448723525\n",
      "Epoch: 10/10 Train batch:30000/31779   acc: 0.717479674796748  loss: 4.582990495022386\n",
      "Save one candidate model.\n",
      "Validation Accuracy: 0.6819894175510035. Cost time: 24.225563311576842 minutes\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "    train_features, train_labels = load_train_data()\n",
    "    val_features, val_labels = load_validation_data()\n",
    "    test_features = load_test_data()\n",
    "    \n",
    "    if pca_available:\n",
    "        '''\n",
    "        load local pca file\n",
    "        '''\n",
    "        pca = pickle.load(open('pca.pkl', 'rb'))\n",
    "    else:\n",
    "        '''\n",
    "        10 features will be enough\n",
    "        '''\n",
    "        pca = PCA(10).fit(np.concatenate(train_features))\n",
    "        pickle.dump(pca, open('pca_{0}_features.pkl'.format(pca.n_components), 'wb'))\n",
    "        \n",
    "    train_features, train_labels = load_and_process_data(train_features, train_labels, pca, context_num)\n",
    "    val_features, val_labels = load_and_process_data(val_features, val_labels, pca, context_num)\n",
    "    \n",
    "    train_context_dataset = ContextDataset(context_num, train_features, train_labels)\n",
    "    val_context_dataset = ContextDataset(context_num, val_features, val_labels)\n",
    "    \n",
    "    '''\n",
    "    It may takes more than 20 minutes to process since it loops over all 15 millions frames\n",
    "    But it could speed up later dataloader process\n",
    "    And I could save this data by pickle\n",
    "    The drawback is it's not flexible to feature engineering, like change context number\n",
    "    '''\n",
    "    '''    \n",
    "    train_dataset = SpeechDataset(train_context_dataset)\n",
    "    val_dataset = SpeechDataset(val_context_dataset)\n",
    "    '''\n",
    "    '''\n",
    "    train_mask = (train_dataset.targets.numpy() != -1)*1\n",
    "    train_sampler = WeightedRandomSampler(weights=train_mask, num_samples=int(train_mask.sum()), replacement=False)\n",
    "    val_mask = (val_dataset.targets.numpy() != -1)*1\n",
    "    val_sampler = WeightedRandomSampler(weights=val_mask, num_samples=int(val_mask.sum()), replacement=False)\n",
    "    '''\n",
    "    '''    \n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                              shuffle = True,\n",
    "                              batch_size = 512,\n",
    "                              num_workers = 0,\n",
    "                              pin_memory = True)\n",
    "    \n",
    "    val_dataloader = DataLoader(val_dataset,\n",
    "                              shuffle = True,\n",
    "                              batch_size = 512,\n",
    "                              num_workers = 0,\n",
    "                              pin_memory = True)\n",
    "    '''\n",
    "    '''\n",
    "    Start train model.\n",
    "    15 epochs by default.\n",
    "    Cost about 1 hour.\n",
    "    '''\n",
    "    \n",
    "    train_dataloader = DataLoader(train_context_dataset,\n",
    "                              shuffle = True,\n",
    "                              batch_size = 512,\n",
    "                              num_workers = 0,\n",
    "                              pin_memory = True)\n",
    "    \n",
    "    val_dataloader = DataLoader(val_context_dataset,\n",
    "                                  shuffle = True,\n",
    "                                  batch_size = 512,\n",
    "                                  num_workers = 0,\n",
    "                                  pin_memory = True)\n",
    "\n",
    "    model = train_model(train_dataloader, val_dataloader)\n",
    "    \n",
    "    pickle.dump(model, open(\"submission_model_68.pkl\", \"wb\"))\n",
    "    #make_submission(model, test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(test_features, model):\n",
    "    \n",
    "    test_false_labels = np.array([np.zeros((test_features[i].shape[0], 1)).reshape(-1) for i in range(len(test_features))])\n",
    "    test_features, test_labels = load_and_process_data(test_features, test_false_labels, pca, context_num)\n",
    "    '''\n",
    "    generate false labels(just used to skip padding frame)\n",
    "    '''\n",
    "    test_context_dataset = ContextDataset(context_num,test_features, test_labels)\n",
    "    test_dataloader = DataLoader(test_context_dataset,\n",
    "                              shuffle = False,\n",
    "                              batch_size = 512,\n",
    "                              num_workers = 0,\n",
    "                              pin_memory = True,\n",
    "                              drop_last = False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval().cuda()\n",
    "        sub_predictions = []\n",
    "        for idx, (index, features, labels) in enumerate(test_dataloader):\n",
    "            mask = [i for i in range(len(labels)) if labels[i] != torch.Tensor([-1]).long()[0]]\n",
    "            features = features[mask].cuda()\n",
    "            outputs= model(features.cuda())\n",
    "            predictions = torch.max(outputs.data, 1)[1] \n",
    "            sub_predictions.append(predictions)\n",
    "        sub_predictions = torch.cat(sub_predictions).flatten().cpu().numpy()\n",
    "    \n",
    "    submission = pd.DataFrame(sub_predictions, columns = ['label'])\n",
    "    submission.to_csv(\"Submission.csv\", index_label = 'id')\n",
    "    print(\"Submission.csv done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading test data...\n",
      "Done loading test data in 0.005717750390370687 minutes...\n",
      "Submission.csv done.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    test_features = load_test_data()\n",
    "    if pca_available:\n",
    "        '''\n",
    "        load local pca file\n",
    "        '''\n",
    "        pca = pickle.load(open('pca.pkl', 'rb'))\n",
    "    else:\n",
    "        '''\n",
    "        10 features will be enough\n",
    "        '''\n",
    "        pca = PCA(10).fit(np.concatenate(train_features))\n",
    "        pickle.dump(pca, open('pca_{0}_features.pkl'.format(pca.n_components), 'wb'))\n",
    "    model = pickle.load(open(\"submission_model_68.pkl\", \"rb\"))\n",
    "    make_submission(test_features, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
