{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.decomposition import PCA\n",
    "import torch.utils.data\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import gc\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "set parameters\n",
    "'''\n",
    "base_path = '.'\n",
    "train_path = base_path + '/train.npy'\n",
    "test_path = base_path + '/test.npy'\n",
    "train_labels_path = base_path + '/train_labels.npy'\n",
    "dev_labels_path = base_path +  '/dev_labels.npy'\n",
    "dev_path = base_path + '/dev.npy'\n",
    "\n",
    "pca_available = True\n",
    "padding_method = 'self' \n",
    "# padding_method = 'zero' \n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "n_labels = 138\n",
    "n_features = 40\n",
    "n_epoch = 10\n",
    "context_num = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data():\n",
    "    t0 = time.time()\n",
    "    print(\"Start loading training data...\")\n",
    "    train = np.load(train_path, allow_pickle=True)\n",
    "    train_labels = np.load(train_labels_path, allow_pickle=True)\n",
    "    t1 = time.time()\n",
    "    elapsed_time = t1 - t0\n",
    "    print(\"Done loading training data in {0} minutes...\".format(elapsed_time/60))\n",
    "    \n",
    "    return train, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_validation_data():\n",
    "    t0 = time.time()\n",
    "    print(\"Start loading validation data...\")\n",
    "    val = np.load(dev_path, allow_pickle=True)\n",
    "    val_labels = np.load(dev_labels_path, allow_pickle=True)\n",
    "    t1 = time.time()\n",
    "    elapsed_time = t1 - t0\n",
    "    print(\"Done loading validation data in {0} minutes...\".format(elapsed_time/60))\n",
    "    \n",
    "    return val, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data():\n",
    "    t0 = time.time()\n",
    "    print(\"Start loading test data...\")\n",
    "    test = np.load(test_path, allow_pickle=True)\n",
    "    t1 = time.time()\n",
    "    elapsed_time = t1 - t0\n",
    "    print(\"Done loading test data in {0} minutes...\".format(elapsed_time/60))\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(features, labels, pca, context_num):\n",
    "    '''\n",
    "    use the first&last frame of one utterance to pad the empty frame\n",
    "    '''\n",
    "    t0 = time.time()\n",
    "    padding_features = np.concatenate([np.concatenate(( \\\n",
    "                                                 np.ones((context_num, pca.n_components))*pca.transform(features[i][[0]]), \\\n",
    "                                                 pca.transform(features[i]), \\\n",
    "                                                 np.ones((context_num, pca.n_components))*pca.transform(features[i][[-1]]))) \\\n",
    "                                                 for i in range(len(features))])\n",
    "    padding_features = torch.Tensor(padding_features)\n",
    "    del features\n",
    "    \n",
    "    '''\n",
    "    corresponding label for padding frames\n",
    "    '''\n",
    "    false_labels = np.array([-1]*context_num)\n",
    "    padding_labels = np.concatenate([np.concatenate(( \\\n",
    "                                                  false_labels, \\\n",
    "                                                  labels[i], \\\n",
    "                                                  false_labels)) \\\n",
    "                                                  for i in range(len(labels))])\n",
    "    padding_labels = torch.Tensor(padding_labels)\n",
    "    del labels\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return padding_features, padding_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, context_num, features, targets):\n",
    "        \n",
    "        self.context_num = context_num\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        if index-self.context_num >= 0 and index+self.context_num+1 <= len(self.targets)-1:\n",
    "            '''\n",
    "            no need for padding\n",
    "            '''\n",
    "            X = self.features[index-self.context_num: index+self.context_num+1].reshape(-1)\n",
    "            Y = self.targets[index].long()\n",
    "        elif index-self.context_num < 0:\n",
    "            '''\n",
    "            padding for pre frames, actually doesnt matter since we drop this 'false' frame\n",
    "            '''\n",
    "            X = torch.cat((torch.zeros(self.context_num-index, self.features.shape[1]), self.features[:index+self.context_num+1]), 0).reshape(-1)\n",
    "            Y = self.targets[index].long()\n",
    "        else:\n",
    "            '''\n",
    "            padding for post frames, same as before\n",
    "            '''\n",
    "            X = torch.cat((self.features[index-self.context_num:], torch.zeros(index+self.context_num+1-len(self.targets), self.features.shape[1])),0).reshape(-1)\n",
    "            Y = self.targets[index].long()\n",
    "        \n",
    "        return index, X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, speechdataset):\n",
    "        \n",
    "        self.features = [speechdataset[i][1] for i in range(len(speechdataset)) if speechdataset[i][2] != torch.Tensor([-1])[0]]\n",
    "        self.targets = [speechdataset[i][2] for i in range(len(speechdataset)) if speechdataset[i][2] != torch.Tensor([-1])[0]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return index, self.features[index], self.targets[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1000,2048,1024,512,256+2,138\n",
    "class SpeechNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, context_num):\n",
    "        \n",
    "        super(SpeechNet, self).__init__()\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.relu7 = nn.ReLU()\n",
    "        self.relu8 = nn.ReLU()\n",
    "        \n",
    "        self.linear1 = nn.Linear((2*context_num+1)*pca.n_components, 2048)\n",
    "        self.linear2 = nn.Linear(2048, 1024)\n",
    "        self.linear3 = nn.Linear(1024, 810)\n",
    "        self.linear4 = nn.Linear(810, 720)\n",
    "        self.linear5 = nn.Linear(720, 512)\n",
    "        self.linear6 = nn.Linear(512, 428)\n",
    "        self.linear7 = nn.Linear(428, 300)\n",
    "        self.linear8 = nn.Linear(300, 256)\n",
    "        self.out = nn.Linear(256+2, 138)\n",
    "        \n",
    "        self.batchnorm1 = nn.BatchNorm1d(2048)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(1024)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(810)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(720)\n",
    "        self.batchnorm5 = nn.BatchNorm1d(512)\n",
    "        self.batchnorm6 = nn.BatchNorm1d(428)\n",
    "        self.batchnorm7 = nn.BatchNorm1d(300)\n",
    "        self.batchnorm8 = nn.BatchNorm1d(256+2)\n",
    "        \n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.05)\n",
    "        self.dropout3 = nn.Dropout(0.1)\n",
    "        self.dropout4 = nn.Dropout(0.05)\n",
    "        self.dropout5 = nn.Dropout(0.05)\n",
    "        self.dropout6 = nn.Dropout(0.05)\n",
    "        self.dropout7 = nn.Dropout(0.05)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu1(x)\n",
    "        #2048\n",
    "        x = self.dropout1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu2(x)\n",
    "        #1024\n",
    "        x = self.dropout2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.relu3(x)\n",
    "        #810\n",
    "        x = self.dropout3(x)\n",
    "        x = self.linear4(x)\n",
    "        x = self.batchnorm4(x)\n",
    "        x = self.relu4(x)\n",
    "        \n",
    "        #512\n",
    "        x = self.dropout4(x)\n",
    "        x = self.linear5(x)\n",
    "        x = self.batchnorm5(x)\n",
    "        x = self.relu5(x)\n",
    "        \n",
    "        #512\n",
    "        x = self.dropout5(x)\n",
    "        x = self.linear6(x)\n",
    "        x = self.batchnorm6(x)\n",
    "        x = self.relu6(x)\n",
    "        #428\n",
    "        x = self.dropout6(x)\n",
    "        x = self.linear7(x)\n",
    "        x = self.batchnorm7(x)\n",
    "        x = self.relu7(x)\n",
    "        #300 \n",
    "        x = self.dropout7(x)\n",
    "        x = self.linear8(x)\n",
    "        #300\n",
    "        \n",
    "        avg_pool1 = torch.mean(x, 1, keepdims = True)\n",
    "        max_pool1,_ = torch.max(x, 1, keepdims = True)\n",
    "        \n",
    "        conc = torch.cat((x, avg_pool1, max_pool1), 1)\n",
    "        conc = self.batchnorm8(conc)\n",
    "        output = self.out(conc)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(context_num, features, labels):\n",
    "    t0 = time.time()\n",
    "    print(\"It may takes 20 minutes to generate train dataset...\")\n",
    "    context_dataset = ContextDataset(context_num, features, labels)\n",
    "    dataset = SpeechDataset(context_dataset)\n",
    "    t1 = time.time()\n",
    "    print(\"Dataset generated. Elapsed time: {0}\".format((t1-t0)/60))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        xavier(m.weight.data)\n",
    "        xavier(m.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_cos(x):\n",
    "    start = 5e-3\n",
    "    end = 1e-5\n",
    "    return start + (1 + np.cos(np.pi * (1 - x))) * (end - start) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_scale_cos(x):\n",
    "    start = 1e-4\n",
    "    end = 1e-8\n",
    "    return start + (1 + np.cos(np.pi * (1 - x))) * (end - start) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamScheduler:\n",
    "    \n",
    "    def __init__(self, optimizer, scale_fn, total_steps):\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.scale_fn = scale_fn\n",
    "        self.total_steps = total_steps\n",
    "        self.current_iteration = 0\n",
    "        \n",
    "    def batch_step(self):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = self.scale_fn(self.current_iteration/self.total_steps)\n",
    "        \n",
    "        self.current_iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataloader, val_dataloader, n_epochs = 15):\n",
    "    \n",
    "    model = SpeechNet(context_num).to(device)\n",
    "    model.apply(weights_init)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "    \n",
    "    '''\n",
    "    set scheduler for decaying learning rate\n",
    "    '''\n",
    "    parameter_scheduler = ParamScheduler(optimizer, scale_cos, n_epoch*len(train_dataloader))\n",
    "    candidate_model = 1\n",
    "    print('Start training...')\n",
    "    for i in range(n_epochs):\n",
    "        \n",
    "        t0 = time.time()\n",
    "        \n",
    "        avg_loss_1000_batch = 0\n",
    "        val_correct = 0\n",
    "        val_predicted = 0\n",
    "        model.train()\n",
    "        \n",
    "        for index, (idx, features, labels) in enumerate(train_dataloader):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            mask = [i for i in range(len(labels)) if labels[i] != torch.Tensor([-1]).long()[0]]\n",
    "            features = features[mask].cuda()\n",
    "            labels = labels[mask].cuda()\n",
    "            '''\n",
    "            forward and backward\n",
    "            '''\n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels.long())\n",
    "            avg_loss_1000_batch += loss.item()\n",
    "            loss.backward()\n",
    "            \n",
    "            parameter_scheduler.batch_step()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if index % 2000 == 0 and index != 0:\n",
    "                \n",
    "                predictions = torch.max(output.data, 1)[1]\n",
    "                predicted = len(features)\n",
    "                correct = int(sum(predictions == labels.to(device)).cpu())\n",
    "                print(\"Epoch: {0}/{1} Train batch:{2}/{3}   acc: {4}  loss: {5}\".format(i+1, \\\n",
    "                                                                              n_epochs, \\\n",
    "                                                                              index, \\\n",
    "                                                                              len(train_dataloader), \\\n",
    "                                                                              correct/predicted, avg_loss_1000_batch/512))\n",
    "                avg_loss_1000_batch = 0\n",
    "                \n",
    "        for index, (idx, val_features, val_labels) in enumerate(val_dataloader):\n",
    "            \n",
    "            mask = [i for i in range(len(val_labels)) if val_labels[i] != torch.Tensor([-1])[0]]\n",
    "            val_features = val_features[mask].to(device)\n",
    "            val_labels = val_labels[mask].to(device)\n",
    "            model.eval()\n",
    "            outputs = model(val_features)\n",
    "            predictions = torch.max(outputs.data, 1)[1]\n",
    "            val_predicted += len(val_features)\n",
    "            val_correct += sum(predictions == val_labels.to(device))\n",
    "            \n",
    "        epoch_acc = int(val_correct.cpu())/val_predicted\n",
    "        if epoch_acc >= 0.70:\n",
    "            pickle.dump(model, open(\"candidate_model_{0}.pkl\".format(candidate_model), 'wb'))\n",
    "            print(\"Save one candidate model.\")\n",
    "            candidate_model += 1\n",
    "            \n",
    "        t1 = time.time()\n",
    "        print(\"Validation Accuracy: {0}. Cost time: {1} minutes\".format(int(val_correct.cpu())/val_predicted, (t1-t0)/60))\n",
    "        print(\"===================================================\")\n",
    "        \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading training data...\n",
      "Done loading training data in 0.7778591275215149 minutes...\n",
      "Start loading validation data...\n",
      "Done loading validation data in 0.04492399295171102 minutes...\n",
      "Start loading test data...\n",
      "Done loading test data in 0.014317631721496582 minutes...\n",
      "Start training...\n",
      "Epoch: 1/15 Train batch:2000/30057   acc: 0.435546875  loss: 9.695134555222467\n",
      "Epoch: 1/15 Train batch:4000/30057   acc: 0.525390625  loss: 8.058894955553114\n",
      "Epoch: 1/15 Train batch:6000/30057   acc: 0.50390625  loss: 7.55276261921972\n",
      "Epoch: 1/15 Train batch:8000/30057   acc: 0.51171875  loss: 7.251677069114521\n",
      "Epoch: 1/15 Train batch:10000/30057   acc: 0.552734375  loss: 6.99999346002005\n",
      "Epoch: 1/15 Train batch:12000/30057   acc: 0.568359375  loss: 6.822190935024992\n",
      "Epoch: 1/15 Train batch:14000/30057   acc: 0.578125  loss: 6.700407363474369\n",
      "Epoch: 1/15 Train batch:16000/30057   acc: 0.529296875  loss: 6.569748811190948\n",
      "Epoch: 1/15 Train batch:18000/30057   acc: 0.57421875  loss: 6.4731716946698725\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-7b567fd57a0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mCost\u001b[0m \u001b[0mabout\u001b[0m \u001b[1;36m1\u001b[0m \u001b[0mhour\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     '''\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;31m#make_submission(model, test_features)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-bf22d7e15679>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(n_epochs)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python3.5\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python3.5\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\python3.5\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python3.5\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python3.5\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'numpy'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'string_'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    train_features, train_labels = load_train_data()\n",
    "    val_features, val_labels = load_validation_data()\n",
    "    test_features = load_test_data()\n",
    "    \n",
    "    if pca_available:\n",
    "        '''\n",
    "        load local pca file\n",
    "        '''\n",
    "        pca = pickle.load(open('pca.pkl', 'rb'))\n",
    "    else:\n",
    "        '''\n",
    "        10 features will be enough\n",
    "        '''\n",
    "        pca = PCA(10).fit(np.concatenate(train_features))\n",
    "        pickle.dump(pca, open('pca_{0}_features.pkl'.format(pca.n_components), 'wb'))\n",
    "        \n",
    "    train_features, train_labels = load_and_process_data(train_features, train_labels, pca, context_num)\n",
    "    val_features, val_labels = load_and_process_data(val_features, val_labels, pca, context_num)\n",
    "    \n",
    "    train_context_dataset = ContextDataset(context_num, train_features, train_labels)\n",
    "    val_context_dataset = ContextDataset(context_num, val_features, val_labels)\n",
    "    \n",
    "    '''\n",
    "    It may takes more than 20 minutes to process since it loops over all 15 millions frames\n",
    "    But it could speed up later dataloader process\n",
    "    And I could save this data by pickle\n",
    "    The drawback is it's not flexible to feature engineering, like change context number\n",
    "    '''\n",
    "    '''    \n",
    "    train_dataset = SpeechDataset(train_context_dataset)\n",
    "    val_dataset = SpeechDataset(val_context_dataset)\n",
    "    '''\n",
    "    '''\n",
    "    train_mask = (train_dataset.targets.numpy() != -1)*1\n",
    "    train_sampler = WeightedRandomSampler(weights=train_mask, num_samples=int(train_mask.sum()), replacement=False)\n",
    "    val_mask = (val_dataset.targets.numpy() != -1)*1\n",
    "    val_sampler = WeightedRandomSampler(weights=val_mask, num_samples=int(val_mask.sum()), replacement=False)\n",
    "    '''\n",
    "    '''    \n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                              shuffle = True,\n",
    "                              batch_size = 512,\n",
    "                              num_workers = 0,\n",
    "                              pin_memory = True)\n",
    "    \n",
    "    val_dataloader = DataLoader(val_dataset,\n",
    "                              shuffle = True,\n",
    "                              batch_size = 512,\n",
    "                              num_workers = 0,\n",
    "                              pin_memory = True)\n",
    "    '''\n",
    "    '''\n",
    "    Start train model.\n",
    "    15 epochs by default.\n",
    "    Cost about 1 hour.\n",
    "    '''\n",
    "    \n",
    "    train_dataloader = DataLoader(train_context_dataset,\n",
    "                              shuffle = True,\n",
    "                              batch_size = 512,\n",
    "                              num_workers = 0,\n",
    "                              pin_memory = True)\n",
    "    \n",
    "    val_dataloader = DataLoader(val_context_dataset,\n",
    "                                  shuffle = True,\n",
    "                                  batch_size = 512,\n",
    "                                  num_workers = 0,\n",
    "                                  pin_memory = True)\n",
    "\n",
    "    model = train_model(train_dataloader, val_dataloader)\n",
    "    \n",
    "    pickle.dump(model, open(\"submission_model_68.pkl\", \"wb\"))\n",
    "    #make_submission(model, test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_context_dataset = ContextDataset(context_num, train_features, train_labels)\n",
    "val_context_dataset = ContextDataset(context_num, val_features, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_context_dataset,\n",
    "                              shuffle = True,\n",
    "                              batch_size = 512,\n",
    "                              num_workers = 0,\n",
    "                              pin_memory = True)\n",
    "    \n",
    "val_dataloader = DataLoader(val_context_dataset,\n",
    "                              shuffle = True,\n",
    "                              batch_size = 512,\n",
    "                              num_workers = 0,\n",
    "                              pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_epochs = 15):\n",
    "    \n",
    "    model = SpeechNet(context_num).to(device)\n",
    "    model.apply(weights_init)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "    \n",
    "    '''\n",
    "    set scheduler for decaying learning rate\n",
    "    '''\n",
    "    parameter_scheduler = ParamScheduler(optimizer, scale_cos, n_epoch*len(train_dataloader))\n",
    "    candidate_model = 1\n",
    "    print('Start training...')\n",
    "    for i in range(n_epochs):\n",
    "        \n",
    "        t0 = time.time()\n",
    "        \n",
    "        avg_loss_1000_batch = 0\n",
    "        val_correct = 0\n",
    "        val_predicted = 0\n",
    "        model.train()\n",
    "        \n",
    "        for index, (idx, features, labels) in enumerate(train_dataloader):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            mask = [i for i in range(len(labels)) if labels[i] != torch.Tensor([-1]).long()[0]]\n",
    "            features = features[mask].cuda()\n",
    "            labels = labels[mask].cuda()\n",
    "            '''\n",
    "            forward and backward\n",
    "            '''\n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels.long())\n",
    "            avg_loss_1000_batch += loss.item()\n",
    "            loss.backward()\n",
    "            \n",
    "            parameter_scheduler.batch_step()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if index % 2000 == 0 and index != 0:\n",
    "                \n",
    "                predictions = torch.max(output.data, 1)[1]\n",
    "                predicted = len(features)\n",
    "                correct = int(sum(predictions == labels.to(device)).cpu())\n",
    "                print(\"Epoch: {0}/{1} Train batch:{2}/{3}   acc: {4}  loss: {5}\".format(i+1, \\\n",
    "                                                                              n_epochs, \\\n",
    "                                                                              index, \\\n",
    "                                                                              len(train_dataloader), \\\n",
    "                                                                              correct/predicted, avg_loss_1000_batch/512))\n",
    "                avg_loss_1000_batch = 0\n",
    "                \n",
    "        for index, (idx, val_features, val_labels) in enumerate(val_dataloader):\n",
    "            \n",
    "            mask = [i for i in range(len(val_labels)) if val_labels[i] != torch.Tensor([-1])[0]]\n",
    "            val_features = val_features[mask].to(device)\n",
    "            val_labels = val_labels[mask].to(device)\n",
    "            model.eval()\n",
    "            outputs = model(val_features)\n",
    "            predictions = torch.max(outputs.data, 1)[1]\n",
    "            val_predicted += len(val_features)\n",
    "            val_correct += sum(predictions == val_labels.to(device))\n",
    "            \n",
    "        epoch_acc = int(val_correct.cpu())/val_predicted\n",
    "        if epoch_acc >= 0.70:\n",
    "            pickle.dump(model, open(\"candidate_model_{0}.pkl\".format(candidate_model), 'wb'))\n",
    "            print(\"Save one candidate model.\")\n",
    "            candidate_model += 1\n",
    "            \n",
    "        t1 = time.time()\n",
    "        print(\"Validation Accuracy: {0}. Cost time: {1} minutes\".format(int(val_correct.cpu())/val_predicted, (t1-t0)/60))\n",
    "        print(\"===================================================\")\n",
    "        \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-51cd4308ec23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-89-265f635718ed>\u001b[0m in \u001b[0;36mtrain_model_1\u001b[1;34m(n_epochs)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mforward\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             '''\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mavg_loss_1000_batch\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python3.5\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-16db163a127c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatchnorm3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;31m#810\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python3.5\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python3.5\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python3.5\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   1668\u001b[0m     return torch.batch_norm(\n\u001b[0;32m   1669\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1670\u001b[1;33m         \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1671\u001b[0m     )\n\u001b[0;32m   1672\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train_model_1(n_epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
